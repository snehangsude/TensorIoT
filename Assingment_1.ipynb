{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c0f974-b125-4b41-9765-e0db06b06786",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Problem Statement Explanation:\n",
    "\n",
    "For this assignment, we have to download the data from [here](http://jmcauley.ucsd.edu/data/amazon/links.html), any reviews file that has at least a million reviews.<br>\n",
    "I've chosen to use the \"Movies and TV\" review dataset that contains 1,697,533 reviews.\n",
    "\n",
    "Assigned tasks once the download is done:\n",
    "\n",
    "- Find the item with the least rating\n",
    "- Find the item with the most rating\n",
    "- Find the item with the longest reviews\n",
    "- Finally store it into a parquet file\n",
    "\n",
    "> **To download the dataset, please execute the below cell.<br>Please note that it will download to the current directory you are in and unzip it.**\n",
    "\n",
    "Library Versions used: [requirements.txt](requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cb66eb-0270-46bb-844f-5e18977045e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Movies_and_TV_5.json.gz\"\n",
    "# !gunzip reviews_Movies_and_TV_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba23861-cb4a-4418-83d9-48620738155c",
   "metadata": {},
   "source": [
    "Before running the below cell it is recommended to run the below command in the terminal to create an environment (Linux):\n",
    "\n",
    "```\n",
    "python3 -m venv .env\n",
    "source .env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "<br>\n",
    "\n",
    "Importing Libraries that will be used to answer the above questions. <br>\n",
    "\n",
    "> Using a try-except catch here to make sure if there is an exception *\"ModuleNotFoundError\"* it will run the `requirements.txt` file on the active environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ae9f770-de4e-4a29-ba7c-3b1117ce9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyspark\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql import functions as F\n",
    "    import logging, datetime\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c999a552-67b0-46b6-adbe-aedf7a80213e",
   "metadata": {},
   "source": [
    "Setting up an logger to customize a few logs that are basic checks.\n",
    "\n",
    "> Setting up the logger level at \"INFO\" meaning almost any event will be reported to the log file.<br>\n",
    "> `format` keyword is used to format the logs to a necessary required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919da003-271d-4de9-85de-221973dc239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_log_time = datetime.datetime.now()\n",
    "log_file_path = f\"./movies_{capture_log_time.date()}_at_runtime_{capture_log_time.hour}:{capture_log_time.minute}.log\"\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "logging.info(f\"INITIALIZING Movies script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd34746-38e0-46bf-92ed-3e47040bb00f",
   "metadata": {},
   "source": [
    "## Creation of Class\n",
    "\n",
    "Creating a class `Transformation`. This is to assist us make the code modular and maintainable. I've tried to follow the SOLID principles of Python coding and added docstrings for better understanding. \n",
    "\n",
    "> Due to the various kind of exceptions that can be thrown for a path related method, using a generalized way of catching exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f80354-f464-4a2a-ba6b-86f19a79e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformation:\n",
    "    \"\"\"\n",
    "    Provides methods for common data transformation tasks using a SparkSession.\n",
    "    Args:\n",
    "        spark (SparkSession): The SparkSession to use for transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        logging.info(f\"== Spark session started ==\")\n",
    "\n",
    "    def exit_spark(self):\n",
    "        logging.info(f\"== Spark session stopped ==\")\n",
    "        self.spark.stop()\n",
    "\n",
    "    def json_reader(self, path:str):\n",
    "        \"\"\"\n",
    "        Reads a JSON file into a Spark DataFrame.\n",
    "        Args:\n",
    "            path (str): The path to the JSON file.\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame containing the JSON data.\n",
    "        Raises:\n",
    "            Exception: If an error occurs during reading.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.spark.read.json(path)\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"ERROR: {e} with {path}\")\n",
    "\n",
    "    def parquet_writer(self, df, path:str):\n",
    "        \"\"\"\n",
    "        Writes a Spark DataFrame to a Parquet file.\n",
    "        Args:\n",
    "            df (DataFrame): The DataFrame to write.\n",
    "            path (str): The path to the output Parquet file.\n",
    "        Raises:\n",
    "            Exception: If an error occurs during writing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return df.repartition(1).write.mode('overwrite').parquet(path)\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"ERROR: {e} with {path}\")\n",
    "            \n",
    "    def transform_time(self, df, col_name:str, format_out:str, format_in=None, unix=False):\n",
    "        \"\"\"\n",
    "        Transforms a time-related column in a DataFrame.\n",
    "        Args:\n",
    "            df: The DataFrame containing the column to transform.\n",
    "            col_name: The name of the column to transform.\n",
    "            format_out: The desired output format for the transformed column.\n",
    "            format_in (optional): The input format of the column, if not Unix timestamp. Defaults to None.\n",
    "            unix (optional): Whether the column is in Unix timestamp format. Defaults to False.\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with the transformed column.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Transforming {col_name} to format {format_out} and isUnix is {unix}\")\n",
    "        if not unix and format_in != None:\n",
    "            return df.withColumn(col_name , F.date_format(F.to_date(F.col(col_name), format_in), format_out))\n",
    "        elif unix:\n",
    "            return df.withColumn(col_name, F.date_format(F.from_unixtime(col_name), format_out))\n",
    "        else:\n",
    "            raise Exception(\"For unix=False, 'format_in' is necessary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fad3db-b584-4416-a727-9a198b78908b",
   "metadata": {},
   "source": [
    "## Spark Session Initialization\n",
    "\n",
    "- `spark.executor.memory`: Used for data processing and computations.\n",
    "- `spark.driver.memory`: Used for managing jobs\n",
    "- `spark.executor.cores`: Used for parallel processing\n",
    "- `spark.executor.instances`: Used for better processing capacity\n",
    "- `spark.sql.legacy.timeParserPolicy`: Used for datetime related bugs to not arise\n",
    "\n",
    "> **Note:** If you want console logs to be printed set it on `INFO` level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a125d59-a080-46d0-9c66-028470faaa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/25 20:53:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"amz_reviews\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"5\") \\\n",
    "    .config(\"spark.executor.instances\", \"5\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\") # Use FATAL for cleaner notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec919d5-8d95-4350-bc27-3037363348a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Transformation(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc64276-a1ef-4f07-be71-970ded40f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movies_df = processor.json_reader(\"./raw_lake/reviews_Movies_and_TV_5.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801080a3-03cf-4fac-99ff-e6a43fb938ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = processor.transform_time(movies_df, 'reviewTime', format_in='MM dd, yyyy', format_out='MM-dd-yyyy')\n",
    "movies_df = processor.transform_time(movies_df, 'unixReviewTime', format_out='MM-dd-yyyy', unix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10d4bbb5-11f1-4984-bef0-cce928699967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb4d5f-b6b5-44a7-8894-550142c19852",
   "metadata": {},
   "source": [
    "## Description of the columns\n",
    "\n",
    "`asin` - ID of the product, e.g. 0000013714<br>\n",
    "`helpful` - helpfulness rating of the review, e.g. 2/3<br>\n",
    "`overall` - rating of the product<br>\n",
    "`reviewText` - text of the review<br>\n",
    "`reviewTime` - time of the review (raw)<br>\n",
    "`reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B<br>\n",
    "`reviewerName` - name of the reviewer<br>\n",
    "`summary` - summary of the review<br>\n",
    "`unixReviewTime` - time of the review (unix time)\n",
    "\n",
    "## Least and Most reviews\n",
    "\n",
    "To find the least or the most reviews we need to *group by* `asin` as it's the ID of the product (unique value)\n",
    "<br>Then *count* the number of reviews for each `asin`\n",
    "<br>Then *sort* them in ascending for least reviews and descending for most reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de6e08d-a334-4c27-a0ea-a038064d5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_reviews = movies_df.groupBy(\"asin\").count().sort(F.asc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee599ea2-cdf3-4829-9d1f-2cfc2ced2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_reviews = movies_df.groupBy(\"asin\").count().sort(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72bebc7-bc4b-4452-9700-59950448d9e0",
   "metadata": {},
   "source": [
    "## Longest reviews:\n",
    "\n",
    "To find the longest review we choose the `reviewText` column and get the *length* of each given text.\n",
    "<br>Then, we *group by* `asin` \n",
    "<br>Perform two aggregration using the *sum* total of the *length*\n",
    "<br>And finally, *sorting* it in desending order of the length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5483983-d7cb-4be9-99ca-05e693407f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.withColumn('length_summary', F.length(F.col('reviewText')))\n",
    "longest_reviews = movies_df.groupby(\"asin\").agg(F.sum(\"length_summary\").alias(\"total_len\")).sort(F.desc(\"total_len\"))                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057eb55-4ab8-4150-b270-6d1f22302a56",
   "metadata": {},
   "source": [
    "In the above cell we are intentionally not showing the output as spark engine by default prefers to behave with lazy loading enabled. This is what allows spark to enhance performance over large datasets.<br> While keeping the lazy loading active to get the result out we run 3 computation queries using the `first()` method. \n",
    "\n",
    "Once computed -  we create a new dataframe and store the values along with the ASIN ids for a better view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4466237f-efe4-46c8-bd66-7146a22d5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+----------------+--------------------+\n",
      "|   ref|least_movies_asin|most_movies_asin|longest_reviews_asin|\n",
      "+------+-----------------+----------------+--------------------+\n",
      "|  asin|       0780018648|      B003EYVXV4|          B00003CWT6|\n",
      "|values|                5|            2213|             1553600|\n",
      "+------+-----------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Executing data computation: {datetime.datetime.now()}\")\n",
    "results = least_reviews.first(), most_reviews.first(), longest_reviews.first()\n",
    "logging.info(f\"Computation end time: {datetime.datetime.now()}\")\n",
    "\n",
    "\n",
    "final_answers = spark.createDataFrame([\n",
    "    (\"asin\", results[0][0], results[1][0], results[2][0]),\n",
    "    (\"values\", results[0][1], results[1][1], results[2][1])\n",
    "], \"ref string, least_movies_asin string, most_movies_asin string, longest_reviews_asin string\")\n",
    "\n",
    "final_answers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98dc23-8be0-4227-9026-1d7b699b6d0e",
   "metadata": {},
   "source": [
    "## A Desired Operation\n",
    "\n",
    "Here I've used `when` from the functions module.<br>\n",
    "This specifically serves as an `if-else` for any respective column helping us better segregate and put logical conclusions over data.<br>\n",
    "`helpful` is a array with the index 0 element being helpful ones and index 1 element being the total reactions. This will help us understand which reviews are actually helpful to the customer and if there are specific reviews that can be deep-dived more. \n",
    "We create a column `effective rating`. <br>\n",
    "- Any value below 1000 is called as basic\n",
    "- Between 1000 to 5000 as intermediate\n",
    "- Above 5000 as advanced\n",
    "\n",
    "We can then better understand the data, or create available ready features for ML teams to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea49c179-3256-428e-b5f7-f781ea527e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df.withColumn(\"total_votes\", F.col(\"helpful\")[1])\n",
    "movies_df = movies_df.withColumn(\"actual_helpful\", F.col(\"helpful\")[0])\n",
    "movies_df = movies_df.withColumn('effective_rating',\n",
    "                                F.when((F.col(\"actual_helpful\") / F.col(\"total_votes\") * 100) > 60, \"above 60%\") \n",
    "                                .when((F.col(\"actual_helpful\") / F.col(\"total_votes\") * 100) >= 30, \"below 60%\")\n",
    "                                .otherwise(\"below 30%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ee8328-5cb3-4f81-b602-c06582241d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+--------------------+----------+-------------+--------------------+--------------------+--------------+--------------+-----------+--------------+----------------+\n",
      "|      asin|helpful|overall|          reviewText|reviewTime|   reviewerID|        reviewerName|             summary|unixReviewTime|length_summary|total_votes|actual_helpful|effective_rating|\n",
      "+----------+-------+-------+--------------------+----------+-------------+--------------------+--------------------+--------------+--------------+-----------+--------------+----------------+\n",
      "|0005019281| [0, 0]|    4.0|This is a charmin...|02-26-2008|ADZPIG9QOCDG5|Alice L. Larson \"...|good version of a...|    02-26-2008|           299|          0|             0|       below 30%|\n",
      "+----------+-------+-------+--------------------+----------+-------------+--------------------+--------------------+--------------+--------------+-----------+--------------+----------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: string (nullable = true)\n",
      " |-- length_summary: integer (nullable = true)\n",
      " |-- total_votes: long (nullable = true)\n",
      " |-- actual_helpful: long (nullable = true)\n",
      " |-- effective_rating: string (nullable = false)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(movies_df.show(1))\n",
    "print(movies_df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574646cc-1b64-47c1-b605-b5731b59fde5",
   "metadata": {},
   "source": [
    "The below row of code will save the transformed dataframe into a `.parquet` file. \n",
    "\n",
    "Original File Size: `2.0Â GB`<br>\n",
    "Parquest File Size: `1.1 GB`\n",
    "\n",
    "Even with the introduction of new columns the space the data consumes if almost a half less than the original dump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bb0025e-6d5c-449f-a715-c0c8d6c2de70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processor.parquet_writer(movies_df, \"./raw_lake/movies_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4e6c51-faec-47cb-bdd7-f83703d06187",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.unpersist()\n",
    "processor.exit_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc77759-4f34-423a-a500-6981c908949a",
   "metadata": {},
   "source": [
    "# Final Results:\n",
    "\n",
    "1. Item with the least ratings:\n",
    "   > **ASIN: 0780018648** with `5` ratings.\n",
    "3. Item with the most ratings:\n",
    "   > **ASIN: B003EYVXV4** with `2213` ratings.\n",
    "5. Item with the longest reviews:\n",
    "   > **ASIN: B00003CWT6** with accumulative lenght of reviews `1553600` letters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
